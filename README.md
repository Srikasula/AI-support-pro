

# ğŸ¤– AI Support Pro

AI Support Pro is a **full-stack AI-powered customer support system** built with  
**Next.js 14 (frontend)** and **FastAPI (backend)** â€” enabling users to upload their own documents (PDF, TXT, MD)  
and chat with an intelligent assistant that understands and answers from those documents.

---

## ğŸš€ Purpose

This project demonstrates how **Retrieval-Augmented Generation (RAG)** can help businesses build  
AI support assistants that provide accurate, context-aware answers from private data.

---

## ğŸ§  Key Features

- ğŸ“ Upload & index PDF or text documents into a vector store (ChromaDB)
- ğŸ’¬ Ask natural language questions based on your uploaded data
- ğŸ¤– Contextual AI answers with source citations
- âš¡ Real-time streaming using **SSE (Server-Sent Events)**
- ğŸ§© Built with **FastAPI + LangChain + OpenAI API + Next.js**
- ğŸŒˆ Modern Tailwind CSS UI with real-time chat experience

---

## ğŸ§± Tech Stack

| Layer | Technology |
|-------|-------------|
| **Frontend** | Next.js 14, React, Tailwind CSS |
| **Backend** | FastAPI, Python 3.12 |
| **AI / RAG Engine** | LangChain, ChromaDB, Hugging Face Embeddings |
| **Cloud / API** | OpenAI GPT API |
| **Deployment** | Vercel (Frontend) + Render (Backend) |

---

## ğŸ—‚ï¸ Project Structure

AI-support-pro/
â”‚
â”œâ”€â”€ app/ # Next.js frontend
â”‚ â”œâ”€â”€ chat/ # Chat interface
â”‚ â”œâ”€â”€ upload/ # Document upload page
â”‚ â”œâ”€â”€ components/ # Reusable UI components
â”‚ â””â”€â”€ page.tsx # Main chatbot UI
â”‚
â”œâ”€â”€ backend/ # FastAPI backend
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ main.py # API routes (chat, upload, health)
â”‚ â”‚ â”œâ”€â”€ rag.py # RAG logic with LangChain + Chroma
â”‚ â”‚ â”œâ”€â”€ ingest.py # Document ingestion logic
â”‚ â”‚ â”œâ”€â”€ auth.py # Authentication placeholder
â”‚ â”‚ â””â”€â”€ storage.py # File storage utilities
â”‚ â”œâ”€â”€ requirements.txt # Python dependencies
â”‚ â””â”€â”€ .env.example # Backend environment template
â”‚
â”œâ”€â”€ .env.local # Frontend environment
â”œâ”€â”€ .gitignore # Ignored folders/files
â”œâ”€â”€ README.md # Documentation
â””â”€â”€ tailwind.config.ts # Tailwind CSS config


---

## âš™ï¸ Local Setup

### ğŸ§© Backend Setup (FastAPI)

```bash
cd backend
python3 -m venv .venv
source .venv/bin/activate

pip install -r requirements.txt

# Add your OpenAI API Key
echo "OPENAI_API_KEY=sk-your-key" > .env

# Run the backend server
uvicorn app.main:app --reload --port 8000

Visit ğŸ‘‰ http://127.0.0.1:8000/health
You should see:

{"ok": true}

ğŸ–¥ï¸ Frontend Setup (Next.js)
cd ..
npm install

# Connect the frontend to backend
echo "NEXT_PUBLIC_BACKEND_URL=http://127.0.0.1:8000" > .env.local

# Run frontend
npm run dev


Visit ğŸ‘‰ http://localhost:3000

How It Works

Upload PDF or text documents from the Upload page.

The backend extracts text and creates vector embeddings using LangChain + ChromaDB.

Ask any question in the Chat page.

The assistant searches your vector database and replies with AI-generated answers and citations.

API Testing (with curl)
Health Check
curl http://127.0.0.1:8000/health

Upload Document
curl -X POST http://127.0.0.1:8000/upload \
  -F "files=@/path/to/YourFile.pdf"

Ask a Question
curl -X POST http://127.0.0.1:8000/chat_text \
  -H "Content-Type: application/json" \
  -d '{"query":"What is the refund policy?","session_id":"local"}'

Environment Variables
Backend â†’ backend/.env
OPENAI_API_KEY=your_openai_api_key

Frontend â†’ .env.local
NEXT_PUBLIC_BACKEND_URL=http://127.0.0.1:8000

Deployment Guide
ğŸŒ©ï¸ Deploy Backend on Render

Go to https://render.com

Create New Web Service

Select your GitHub repo

Root directory â†’ backend

Build Command â†’ pip install -r requirements.txt

Start Command â†’ uvicorn app.main:app --host 0.0.0.0 --port 10000

Add environment variable:

OPENAI_API_KEY=your_openai_api_key


Deploy â†’ note the backend URL (e.g., https://ai-support-backend.onrender.com)

Deploy Frontend on Vercel

Go to https://vercel.com

Import your GitHub repository

Add environment variable:

NEXT_PUBLIC_BACKEND_URL=https://ai-support-backend.onrender.com


Click Deploy

Once done â†’ open your Vercel URL and test your chatbot!

ğŸ§¹ .gitignore
node_modules/
.next/
backend/.venv/
backend/.chroma/
backend/ai_support.db
backend/.env
.env.local

ğŸ§  Future Enhancements

ğŸ—‚ï¸ Add advanced PDF & DOCX parsing (PyMuPDF / Unstructured)

ğŸ’¾ Use PostgreSQL or Pinecone for persistent vector storage

ğŸ—£ï¸ Add speech-to-text & text-to-speech

ğŸ‘¥ Enable multi-user authentication (JWT)

ğŸ“Š Add analytics dashboard with chat history

ğŸ‘¨â€ğŸ’» Author

Srikanth Kasula
Senior Full Stack Developer | Dallas, TX

